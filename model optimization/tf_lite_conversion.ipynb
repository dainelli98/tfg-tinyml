{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Conversion a TensorFlow Lite\n",
    "Se convierten los modelos de TensorFlow a TensorFlow Lite, generando en cada caso una versión cuantizada y una versión\n",
    "no cuantizada.\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from tf_lite_conversion import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parámetros\n",
    "Parámetros para ajustar la conversión."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "project_dir = \"/home/daniel/PycharmProjects/tfg-tinyml\"\n",
    "\n",
    "saved_audio_models_dir = f\"{project_dir}/saved models/audio\"\n",
    "audio_model_name = f\"ExtAudioDataModel\"\n",
    "audio_model_dir = f\"{saved_audio_models_dir}/tensorflow\"\n",
    "audio_model_output_dir = f\"{saved_audio_models_dir}/tensorflow lite\"\n",
    "audio_representative_dataset_dir = f\"{project_dir}/samples/external/audio/train\"\n",
    "\n",
    "saved_image_models_dir = f\"{project_dir}/saved models/image\"\n",
    "image_model_name = f\"MicroImgDataModel\"\n",
    "image_model_dir = f\"{saved_image_models_dir}/tensorflow\"\n",
    "image_model_output_dir = f\"{saved_image_models_dir}/tensorflow lite\"\n",
    "image_representative_dataset_dir = f\"{project_dir}/samples/microcontroller/preprocessed image/train\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Carga de datasets representativos\n",
    "Se cargan los datasets que se usarán para ajustar la cuantización."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 274 files belonging to 3 classes.\n",
      "Using 12924 samples.\n",
      "WARNING:tensorflow:From /home/daniel/.local/lib/python3.8/site-packages/tensorflow/python/ops/parallel_for/pfor.py:2380: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample\n"
     ]
    }
   ],
   "source": [
    "image_representative_dataset = get_image_representative_dataset(image_representative_dataset_dir)\n",
    "audio_representative_dataset = get_audio_representative_dataset(audio_representative_dataset_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conversión de modelos\n",
    "Se aplica la conversión de modelos."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted. The model has been saved in /home/daniel/PycharmProjects/tfg-tinyml/saved models/audio/tensorflow lite/ExtAudioDataModel.tflite.\n",
      "Applying quantization to ExtAudioDataModel.\n",
      "Model converted. The model has been saved in /home/daniel/PycharmProjects/tfg-tinyml/saved models/audio/tensorflow lite/ExtAudioDataModelQuant.tflite.\n",
      "Model converted. The model has been saved in /home/daniel/PycharmProjects/tfg-tinyml/saved models/image/tensorflow lite/MicroImgDataModel.tflite.\n",
      "Applying quantization to MicroImgDataModel.\n",
      "tf.Tensor(\n",
      "[[[[[ 37.]\n",
      "    [ 38.]\n",
      "    [ 38.]\n",
      "    ...\n",
      "    [ 25.]\n",
      "    [ 23.]\n",
      "    [ 21.]]\n",
      "\n",
      "   [[ 37.]\n",
      "    [ 38.]\n",
      "    [ 38.]\n",
      "    ...\n",
      "    [ 29.]\n",
      "    [ 27.]\n",
      "    [ 26.]]\n",
      "\n",
      "   [[ 37.]\n",
      "    [ 38.]\n",
      "    [ 38.]\n",
      "    ...\n",
      "    [ 33.]\n",
      "    [ 32.]\n",
      "    [ 31.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[ 50.]\n",
      "    [ 49.]\n",
      "    [ 48.]\n",
      "    ...\n",
      "    [ 50.]\n",
      "    [ 42.]\n",
      "    [ 37.]]\n",
      "\n",
      "   [[ 53.]\n",
      "    [ 51.]\n",
      "    [ 49.]\n",
      "    ...\n",
      "    [ 45.]\n",
      "    [ 38.]\n",
      "    [ 35.]]\n",
      "\n",
      "   [[ 55.]\n",
      "    [ 52.]\n",
      "    [ 49.]\n",
      "    ...\n",
      "    [ 44.]\n",
      "    [ 39.]\n",
      "    [ 37.]]]\n",
      "\n",
      "\n",
      "  [[[164.]\n",
      "    [163.]\n",
      "    [163.]\n",
      "    ...\n",
      "    [ 49.]\n",
      "    [ 59.]\n",
      "    [ 67.]]\n",
      "\n",
      "   [[164.]\n",
      "    [163.]\n",
      "    [163.]\n",
      "    ...\n",
      "    [ 48.]\n",
      "    [ 59.]\n",
      "    [ 67.]]\n",
      "\n",
      "   [[164.]\n",
      "    [164.]\n",
      "    [164.]\n",
      "    ...\n",
      "    [ 48.]\n",
      "    [ 58.]\n",
      "    [ 67.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[173.]\n",
      "    [171.]\n",
      "    [170.]\n",
      "    ...\n",
      "    [ 43.]\n",
      "    [ 47.]\n",
      "    [ 51.]]\n",
      "\n",
      "   [[173.]\n",
      "    [171.]\n",
      "    [169.]\n",
      "    ...\n",
      "    [ 44.]\n",
      "    [ 48.]\n",
      "    [ 52.]]\n",
      "\n",
      "   [[173.]\n",
      "    [171.]\n",
      "    [169.]\n",
      "    ...\n",
      "    [ 45.]\n",
      "    [ 49.]\n",
      "    [ 53.]]]\n",
      "\n",
      "\n",
      "  [[[165.]\n",
      "    [175.]\n",
      "    [179.]\n",
      "    ...\n",
      "    [157.]\n",
      "    [176.]\n",
      "    [197.]]\n",
      "\n",
      "   [[172.]\n",
      "    [164.]\n",
      "    [155.]\n",
      "    ...\n",
      "    [123.]\n",
      "    [147.]\n",
      "    [176.]]\n",
      "\n",
      "   [[188.]\n",
      "    [174.]\n",
      "    [158.]\n",
      "    ...\n",
      "    [101.]\n",
      "    [106.]\n",
      "    [122.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[162.]\n",
      "    [172.]\n",
      "    [179.]\n",
      "    ...\n",
      "    [156.]\n",
      "    [156.]\n",
      "    [200.]]\n",
      "\n",
      "   [[148.]\n",
      "    [178.]\n",
      "    [194.]\n",
      "    ...\n",
      "    [138.]\n",
      "    [146.]\n",
      "    [190.]]\n",
      "\n",
      "   [[157.]\n",
      "    [190.]\n",
      "    [197.]\n",
      "    ...\n",
      "    [130.]\n",
      "    [138.]\n",
      "    [173.]]]\n",
      "\n",
      "\n",
      "  ...\n",
      "\n",
      "\n",
      "  [[[222.]\n",
      "    [216.]\n",
      "    [212.]\n",
      "    ...\n",
      "    [189.]\n",
      "    [186.]\n",
      "    [185.]]\n",
      "\n",
      "   [[217.]\n",
      "    [220.]\n",
      "    [226.]\n",
      "    ...\n",
      "    [191.]\n",
      "    [189.]\n",
      "    [188.]]\n",
      "\n",
      "   [[218.]\n",
      "    [227.]\n",
      "    [238.]\n",
      "    ...\n",
      "    [194.]\n",
      "    [193.]\n",
      "    [192.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[193.]\n",
      "    [207.]\n",
      "    [210.]\n",
      "    ...\n",
      "    [142.]\n",
      "    [142.]\n",
      "    [141.]]\n",
      "\n",
      "   [[208.]\n",
      "    [212.]\n",
      "    [213.]\n",
      "    ...\n",
      "    [142.]\n",
      "    [142.]\n",
      "    [141.]]\n",
      "\n",
      "   [[212.]\n",
      "    [210.]\n",
      "    [209.]\n",
      "    ...\n",
      "    [142.]\n",
      "    [142.]\n",
      "    [141.]]]\n",
      "\n",
      "\n",
      "  [[[172.]\n",
      "    [172.]\n",
      "    [171.]\n",
      "    ...\n",
      "    [126.]\n",
      "    [126.]\n",
      "    [125.]]\n",
      "\n",
      "   [[172.]\n",
      "    [171.]\n",
      "    [171.]\n",
      "    ...\n",
      "    [126.]\n",
      "    [126.]\n",
      "    [125.]]\n",
      "\n",
      "   [[171.]\n",
      "    [171.]\n",
      "    [171.]\n",
      "    ...\n",
      "    [126.]\n",
      "    [126.]\n",
      "    [125.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[175.]\n",
      "    [175.]\n",
      "    [174.]\n",
      "    ...\n",
      "    [124.]\n",
      "    [126.]\n",
      "    [127.]]\n",
      "\n",
      "   [[174.]\n",
      "    [174.]\n",
      "    [173.]\n",
      "    ...\n",
      "    [119.]\n",
      "    [123.]\n",
      "    [126.]]\n",
      "\n",
      "   [[173.]\n",
      "    [173.]\n",
      "    [173.]\n",
      "    ...\n",
      "    [115.]\n",
      "    [120.]\n",
      "    [125.]]]\n",
      "\n",
      "\n",
      "  [[[185.]\n",
      "    [185.]\n",
      "    [182.]\n",
      "    ...\n",
      "    [161.]\n",
      "    [158.]\n",
      "    [155.]]\n",
      "\n",
      "   [[186.]\n",
      "    [184.]\n",
      "    [179.]\n",
      "    ...\n",
      "    [163.]\n",
      "    [161.]\n",
      "    [159.]]\n",
      "\n",
      "   [[187.]\n",
      "    [183.]\n",
      "    [176.]\n",
      "    ...\n",
      "    [159.]\n",
      "    [158.]\n",
      "    [158.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[185.]\n",
      "    [186.]\n",
      "    [187.]\n",
      "    ...\n",
      "    [ 93.]\n",
      "    [ 99.]\n",
      "    [101.]]\n",
      "\n",
      "   [[185.]\n",
      "    [186.]\n",
      "    [187.]\n",
      "    ...\n",
      "    [ 94.]\n",
      "    [100.]\n",
      "    [101.]]\n",
      "\n",
      "   [[185.]\n",
      "    [186.]\n",
      "    [187.]\n",
      "    ...\n",
      "    [ 95.]\n",
      "    [102.]\n",
      "    [102.]]]]], shape=(1, 100, 96, 96, 1), dtype=float32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "tensorflow/lite/kernels/pad.cc:123 op_context.dims <= reference_ops::PadKernelMaxDimensionCount() was not true.Node number 2 (PAD) failed to prepare.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-59a51f1d420a>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m                     representative_dataset=audio_representative_dataset)\n\u001B[1;32m      4\u001B[0m \u001B[0mconvert_saved_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimage_model_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimage_model_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimage_model_output_dir\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m convert_saved_model(image_model_name, image_model_dir, image_model_output_dir, quantize=True,\n\u001B[0m\u001B[1;32m      6\u001B[0m                     representative_dataset=image_representative_dataset)\n",
      "\u001B[0;32m~/PycharmProjects/tfg-tinyml/model optimization/tf_lite_conversion.py\u001B[0m in \u001B[0;36mconvert_saved_model\u001B[0;34m(model_name, model_dir, output_dir, quantize, representative_dataset)\u001B[0m\n\u001B[1;32m     30\u001B[0m         \u001B[0moutput_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mf\"{output_dir}/{model_name}.tflite\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 32\u001B[0;31m     \u001B[0mtf_lite_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconverter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvert\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     33\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"wb\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0moutput_file\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\u001B[0m in \u001B[0;36mconvert\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    919\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    920\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mcalibrate_and_quantize\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 921\u001B[0;31m       \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_calibrate_quantize_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mflags\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    922\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    923\u001B[0m     flags_modify_model_io_type = quant_mode.flags_modify_model_io_type(\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\u001B[0m in \u001B[0;36m_calibrate_quantize_model\u001B[0;34m(self, result, inference_input_type, inference_output_type, activations_type, allow_float)\u001B[0m\n\u001B[1;32m    519\u001B[0m                                                 custom_op_registerers_by_func)\n\u001B[1;32m    520\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_experimental_calibrate_only\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_new_quantizer\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 521\u001B[0;31m       calibrated = calibrate_quantize.calibrate(\n\u001B[0m\u001B[1;32m    522\u001B[0m           self.representative_dataset.input_gen)\n\u001B[1;32m    523\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/calibrator.py\u001B[0m in \u001B[0;36mcalibrate\u001B[0;34m(self, dataset_gen)\u001B[0m\n\u001B[1;32m    170\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0minitialized\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    171\u001B[0m         \u001B[0minitialized\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 172\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_calibrator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPrepare\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0ms\u001B[0m \u001B[0;32min\u001B[0m \u001B[0msample\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    173\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_calibrator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mFeedTensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msample\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    174\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_calibrator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCalibrate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: tensorflow/lite/kernels/pad.cc:123 op_context.dims <= reference_ops::PadKernelMaxDimensionCount() was not true.Node number 2 (PAD) failed to prepare.\n"
     ]
    }
   ],
   "source": [
    "convert_saved_model(audio_model_name, audio_model_dir, audio_model_output_dir)\n",
    "convert_saved_model(audio_model_name, audio_model_dir, audio_model_output_dir, quantize=True,\n",
    "                    representative_dataset=audio_representative_dataset)\n",
    "convert_saved_model(image_model_name, image_model_dir, image_model_output_dir)\n",
    "convert_saved_model(image_model_name, image_model_dir, image_model_output_dir, quantize=True,\n",
    "                    representative_dataset=image_representative_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}